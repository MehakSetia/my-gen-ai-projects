{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpZnt4G4/Hz03EZY6tk6JY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehakSetia/my-gen-ai-projects/blob/main/gen_ai_day_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9TGGvxdvEtD",
        "outputId": "34231589-4da7-42c6-c64c-422ab5c63711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI will revolutionise the way we train, live and work.\n",
            " 'We always love being a part of your life' - Peter Dutton Read more: how China is changing with its new technology to give people on low incomes an extra edge in their lives\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator=pipeline('text-generation',model='openai-community/gpt2')\n",
        "\n",
        "output=generator(\"AI will revolutionise\",repetition_penalty=1.5)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2-medium\")\n",
        "\n",
        "\n",
        "output = generator(\"Explain quantum computing:\",\n",
        "                  num_return_sequences=1,\n",
        "                  temperature=0.7,\n",
        "                   repetition_penalty=1.5)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqs5dSEtwg1N",
        "outputId": "332200d9-c919-46ee-af25-78ad97f4745c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain quantum computing: a philosophical approach\n",
            "\"The underlying problem of the 21st century is not that we don't understand it, but how far do you go to explain what's going on?\" asked Bill Barrett, professor emeritus at Georgia Tech who helped popularize \"quantum computation.\" But he told me this wasn. He believes there are no shortcuts in his view either â€” and therefore they should be implemented with as much precision (and speed) available today for modern systems than before Moore's Law kicked off about 1995 or so [ 1 ]. The main challenge now will rest largely within academia too because most people aren' t sure why their computers need superfast processors anymore; though many think its more economical just run them faster using conventional memory chips instead. However well-designed these new memories might get cheaper... some researchers have proposed even better plans where all CPUs would generate power from an ultrahigh bandwidth radio frequency feed which could theoretically drive anything up into multiple petaflops if needed by any specific application.... I am convinced such designs won 't exist anytime soon anyway unless IBM makes serious efforts toward creating efficient data storage devices capable ooze out information quickly enough without requiring massive amounts ot processing time... At present those applications seem unlikely due both cost/speed considerations -- although still possible - AND lack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "def generated_text(prompt):\n",
        "  generator = pipeline(\"text-generation\", model=\"gpt2-medium\",repetition_penalty=1.5)\n",
        "  return generator(prompt, max_length=200)[0]['generated_text']\n",
        "\n",
        "while True:\n",
        "  user_input=input(\"Enter a prompt or quit to exit: \")\n",
        "  if user_input.lower()=='quit':\n",
        "    break\n",
        "  print(\"\\n\"+generated_text(user_input))\n",
        "\n",
        "# Using gpt2-medium + repetition_penalty for better quality\n",
        "# Will learn advanced tuning in Month 4 (LoRA/fine-tuning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4kJf713jvlw",
        "outputId": "430dd791-af6d-43f6-8991-779d646ad877"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a prompt or quit to exit: write a program to reverse a string\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "write a program to reverse a string\n",
            " (also called \"recursive parsing\") is the process of identifying and reversing words in an input file. The following example creates two files that are both one word long, using recursive matching by first looking up all occurrences: $ cat foo bar baz > print 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
            "Enter a prompt or quit to exit: quit\n"
          ]
        }
      ]
    }
  ]
}
