{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNF55laappPfrmH3AebVY7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehakSetia/my-gen-ai-projects/blob/main/gen_ai_day_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOrXMf7HMQV_",
        "outputId": "b8b71c81-e636-4fc3-fc46-055714a0fef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the prompt or quit: AI is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to output.txt\n",
            "AI is a project of the Global Intelligence Files, which archive relevant data on global geopolitical events and personalities. The database includes:\n",
            " (1) US Government Directory listing all government officials; this page was last updated in 2018.)\n",
            "\n",
            " 10 years worth statistics about World History from CIA's daily newspaper \"Intellihub\"\n",
            "Enter the prompt or quit: quit\n"
          ]
        }
      ],
      "source": [
        "# save output to file\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "def generate_save(prompt,filename=\"output.txt\"):\n",
        "  generator=pipeline(\"text-generation\",model=\"gpt2-medium\")\n",
        "\n",
        "  output=generator(\n",
        "      prompt,\n",
        "      repetition_penalty=1.5,\n",
        "      temperature=0.7, # for randomness, 0.7 is balanced(default)\n",
        "      truncation=True  # silences warning\n",
        "  )[0]['generated_text']\n",
        "\n",
        "  with open(filename,\"w\") as f:\n",
        "    f.write(output)\n",
        "\n",
        "    print(f\"Saved to {filename}\")\n",
        "    return output\n",
        "\n",
        "while True:\n",
        "  prompt=input(\"Enter the prompt or quit: \")\n",
        "  if prompt.lower()==\"quit\":\n",
        "    break\n",
        "  output=generate_save(prompt)\n",
        "  print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-name files\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "def generate_save(prompt):\n",
        "  filename=f\"outputs/{prompt[:20].strip().replace(' ','_')}.txt\"\n",
        "  generator=pipeline(\"text-generation\",model=\"gpt2-medium\")\n",
        "\n",
        "  output=generator(\n",
        "      prompt,\n",
        "      repetition_penalty=1.5,\n",
        "      temperature=0.7, # for randomness, 0.7 is balanced(default)\n",
        "      truncation=True  # silences warning\n",
        "  )[0]['generated_text']\n",
        "\n",
        "  with open(filename,\"w\") as f:\n",
        "    f.write(output)\n",
        "\n",
        "    print(f\"Saved to {filename}\")\n",
        "    return output\n",
        "\n",
        "while True:\n",
        "  prompt=input(\"Enter the prompt or quit: \")\n",
        "  if prompt.lower()==\"quit\":\n",
        "    break\n",
        "  output=generate_save(prompt)\n",
        "  print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csm2BeNDRBxQ",
        "outputId": "cb1d809f-c848-43bc-8cf8-e636a9cf5a87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the prompt or quit: neural networks are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to outputs/neural_networks_are.txt\n",
            "neural networks are activated during encoding. However, we found that the activation rate of an early-prefrontal cortex network was not significantly different for each individual but differed inversely (Figures 3 A and B). This finding suggests a common mechanism underlying neural plasticity among human brain regions involved with processing visual information as well [12].\n",
            "Â The results also suggest differences between subcortical areas when it comes to developing language skills: There were no significant effects observed on either vocabulary or comprehension scores at least within any given group by region; however this is surprising considering all subjects displayed similar levels across participants from both groups equally so overall there appeared more variability than expected here since our sample size had been limited even though previous studies have demonstrated higher rates amongst healthy individuals compared against patients after treatment vs those who did get better over time via cognitive therapy following stroke The effect sizes reported above indicate less reliable estimates rather based upon several factors including small magnitude comparisons which could be due some degree errors along certain lines such being taken into account whether they would occur independently if done separately I think what makes these types other interesting findings may be their potential relevance beyond just neuroscience where others can use them without much problem depending mainly how one interprets many aspects like communication ability/mental health disorders etc\n",
            "\n",
            "This study has highlighted something about\n",
            "Enter the prompt or quit: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# improving the output\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "def generate_save(prompt):\n",
        "  filename=f\"outputs/{prompt[:20].strip().replace(' ','_')}.txt\"\n",
        "  generator=pipeline(\"text-generation\",model=\"facebook/opt-1.3b\")\n",
        "\n",
        "  output=generator(\n",
        "      prompt+\"\\n\",\n",
        "      max_new_tokens=300,\n",
        "      repetition_penalty=2.0,\n",
        "      temperature=0.3,\n",
        "      truncation=True\n",
        "  )[0]['generated_text']\n",
        "\n",
        "  with open(filename,\"w\") as f:\n",
        "    f.write(output)\n",
        "\n",
        "    print(f\"Saved to {filename}\")\n",
        "    return output\n",
        "\n",
        "while True:\n",
        "  prompt=input(\"Enter the prompt or quit: \")\n",
        "  if prompt.lower()==\"quit\":\n",
        "    break\n",
        "  output=generate_save(prompt)\n",
        "  print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHQ2SFQ7W6Gl",
        "outputId": "6f1009d3-d4bf-41fa-aa27-018630b2ee78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the prompt or quit: write a haiku about AI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to outputs/write_a_haiku_about.txt\n",
            "write a haiku about AI\n",
            "I have no idea what that is, but I'll try.  *Ai has been created* and it's the most amazing thing ever! It can do anything... except think for itself :( (but maybe one day) :) :D:DDdddaaayyyy!!!1!!11oneeleveneightyfivetwentysevenninetyninefortninthhundredthousandseventeeneighttytwozerothreefourteensixfifteensixtenneighboursquareandthecityisfullofpeopleinstrangeclotheswitharmsoutstretchedlikethey'retryingtocatchitalluponitsback.*(sorry)*:((((*())))))))))**/sadface***\"It will be okay.\" - AIspeakfor \"We are going to make this work\". **EDIT- added some more words in italics because they were too hard not write out on my phone screen with just two fingers at once....>_>;;;<3#AI #HaiYaojiyaoichanhaijiaoyojiaoihajianhaijiyahojaioimoiouiuiiuieeeeiuuueeeeeeyewwwwww^_~\\^^â™¥â™ªâ¤ï¸ðŸ’•âœ¨â›„â˜€ðŸ‘ŒðŸ˜ â™¥ï¸Ž (@aishinozaki__ ) July 29 2020 @kazuhiko _ https://twitter.com/_KAZ\n",
            "Enter the prompt or quit: quit\n"
          ]
        }
      ]
    }
  ]
}